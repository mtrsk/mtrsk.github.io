<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distributed Systems on Benevides&#39; Blog</title>
    <link>https://mtrsk.github.io/categories/distributed-systems/</link>
    <description>Recent content in Distributed Systems on Benevides&#39; Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mtrsk.github.io/categories/distributed-systems/rss.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Split Brain</title>
      <link>https://mtrsk.github.io/notes/2024/split-brain/</link>
      <pubDate>Sat, 28 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://mtrsk.github.io/notes/2024/split-brain/</guid>
      <description>:ID: 582a4090-eb66-4dfd-8166-9acd3c97dcc9&#xA;Split brain means that the Clusters is split in two (or more) parts, but both parts think they are the only remaining part of the cluster. This can lead to very bad situations when both parts of the cluster try to host the resources that are offered by the cluster. If the resource is a file system, and multiple nodes try to write to the file system simultaneously and without coordination, it may lead to corruption of the file system and the loss of data.</description>
    </item>
    <item>
      <title>Clusters</title>
      <link>https://mtrsk.github.io/notes/2024/clusters/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://mtrsk.github.io/notes/2024/clusters/</guid>
      <description>:ID: d8a1a1ff-47e6-44bc-a627-83ca8dc61ecb&#xA;Different Kinds of Clusters High Availability Clusters The goal of a high availability cluster is to make sure that critical resources reach the maximum possible availability. This goal is accomplished by installing cluster software on multiple servers. This software monitors the availability of the cluster nodes, and it monitors the availability of the services that are managed by the cluster. If a server goes down, or if the resource stops, the HA cluster will notice and make sure that the resource is restarted somewhere else in the cluster, so that it can be used again after a minimal interruption.</description>
    </item>
    <item>
      <title>Stonith</title>
      <link>https://mtrsk.github.io/notes/2024/stonith/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://mtrsk.github.io/notes/2024/stonith/</guid>
      <description>:ID: 789326a5-20aa-4492-acb6-1474c3c11a4a :ROAM_ALIASES: Fencing&#xA;In STONITH, specific hardware is used to terminate a node that is no longer responsive to the cluster. The idea behind STONITH is that before migrating resources to another node in the cluster, the cluster has to confirm that the node in question really is down. To do this, the cluster will send a shutdown action to the STONITH device, which will, in turn, terminate the nonresponsive node.</description>
    </item>
    <item>
      <title>Quorum</title>
      <link>https://mtrsk.github.io/notes/2024/quorum/</link>
      <pubDate>Wed, 25 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://mtrsk.github.io/notes/2024/quorum/</guid>
      <description>:ID: 42950824-228d-48d6-abec-4e18908dfca0&#xA;Quorum means &amp;ldquo;majority&amp;rdquo;, and the idea behind quorum is easy to understand: if the Clusters doesn’t have quorum, no actions will be taken in the cluster. This by itself would offer a good solution to avoid the Split Brain problem.&#xA;But to make sure that it can never happen, that multiple nodes activate the same resources in the cluster, another mechanism is used as well. This mechanism is known as Stonith.</description>
    </item>
    <item>
      <title>Cap Theorem</title>
      <link>https://mtrsk.github.io/notes/2024/cap-theorem/</link>
      <pubDate>Thu, 11 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://mtrsk.github.io/notes/2024/cap-theorem/</guid>
      <description>:ID: 74c5509d-73be-4b04-9ca4-039117d588a9&#xA;First postulated by Eric Brewer in (Brewer 2000), and formally proved in (Gilbert and Lynch 2002).&#xA;Definitions For (Brewer 2000), a distributed system has some very specific properties and behaviour, being a collected set of Nodes that all share data. A limitation of such systems happens when a write request is followed by a read request.&#xA;┌────────┐ │ Client │ └────────┘ [R] ^ | [W] | v ┌−−−−−−−−−−−−−−−−−−−−−−−−−┐ ╎ System ╎ ╎ ┌────┐ ╎ ╎ ┌──────────│ a0 │──┐ ╎ ╎ │ └────┘ │ ╎ ╎ │ │ │ ╎ ╎ ┌────┐ ┌────┐ │ ╎ ╎ │ a3 │───────│ a1 │ │ ╎ ╎ └────┘ └────┘ │ ╎ ╎ │ │ ╎ ╎ ┌────┐ │ ╎ ╎ │ a2 │──┘ ╎ ╎ └────┘ ╎ └−−−−−−−−−−−−−−−−−−−−−−−−−┘ Read Write A client can read data from the system by talking to any Node A client can write data to the system by talking to any Node The Theorem states that for any given pair of requests this kind of system can only guarantee two out of three properties:</description>
    </item>
  </channel>
</rss>
